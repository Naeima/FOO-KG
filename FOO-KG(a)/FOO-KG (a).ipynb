{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) \n",
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Graph, RDF, Namespace, plugin\n",
    "from rdflib.plugins import sparql\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib.parser import Parser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from rdflib import Graph, URIRef, BNode, Literal\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tf_geometric as tfg\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Seri data, I have 10 readings a day every 2 hours. The predictions will be interprated accordingly. \n",
    "# Load RDF data\n",
    "g = rdflib.Graph()\n",
    "g.parse(\"SeriKG.rdf\", format=\"ttl\")  \n",
    "\n",
    "# Define RDF namespaces\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "foo = Namespace(\"https://w3id.org/def/foo#\")  # Replace with your ontology namespace\n",
    "pos = Namespace(\"http://w3.org/2003/01/geo/wgs84_pos#\")\n",
    "\n",
    "# Define a SPARQL query to extract features and labels\n",
    "query = prepareQuery(\"\"\" Select *\n",
    "\n",
    "     WHERE {\n",
    "   ?s      rdf:type      ?type ;\n",
    "           foo:GMTDate   ?date;\n",
    "           foo:GMTTime   ?time;\n",
    "           pos:long      ?long ;\n",
    "           pos:lat       ?lat;\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": rdf, \"foo\": foo, \"pos\": pos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for row in g.query(query):\n",
    "    s_uri = row.type\n",
    "    long = row.long.toPython()\n",
    "    lat = row.lat.toPython()\n",
    "    date = row.date.toPython()\n",
    "    time = row.time.toPython()\n",
    "    # store features as lists\n",
    "    KG_features = [long, lat, date,time]\n",
    "\n",
    "    # Append the features to the features list\n",
    "    features.append(KG_features)\n",
    "\n",
    "    # Use long and lat as labels, depending on your task\n",
    "    KG_labels = [long, lat]\n",
    "    labels.append(KG_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and labels to NumPy arrays\n",
    "features_np = np.array(features, dtype=np.float32)\n",
    "labels_np = np.array(labels, dtype=np.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a temporary set and a test set (80% training and 20% test)\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(features_np, labels_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into training and validation sets (60% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the sizes of the sets\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Test set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep learning model with TensorFlow/Keras\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),  # Adjust the input shape based on your features\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='linear')  # Use 'linear' activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['accuracy'])  # Use mean squared error for regression\n",
    "\n",
    "# Train the model on your regression data\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Access the accuracy values during training\n",
    "training_accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model architecture to a file (PNG format)\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# You can also display the model directly in your Jupyter Notebook or IDE:\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eeefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "\n",
    "print(f'Test RMSE: {rmse}')\n",
    "print(f'Test MAPE: {mape}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - predictions\n",
    "\n",
    "# Create a DataFrame to tabulate residuals\n",
    "residuals_df = pd.DataFrame({\n",
    "    'Actual Longitude': y_test[:, 0],\n",
    "    'Predicted Longitude': predictions[:, 0],\n",
    "    'Longitude Residuals': residuals[:, 0],\n",
    "    'Actual Latitude': y_test[:, 1],\n",
    "    'Predicted Latitude': predictions[:, 1],\n",
    "    'Latitude Residuals': residuals[:, 1]\n",
    "}) \n",
    "\n",
    "# Display the tabulated residuals\n",
    "print(residuals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765dd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot for actual test data (in blue)\n",
    "plt.scatter(y_test[:, 1], y_test[:, 0], color='gray', label='Actual', marker='o', s=100)\n",
    "\n",
    "# Create a scatter plot for predicted data (in red)\n",
    "plt.scatter(predictions[:, 1], predictions[:, 0], color='pink', label='Predicted', marker='o', s=100)\n",
    "\n",
    "# Set axis labels and legend\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to your CSV file and the ontology file\n",
    "csv_file = \"Seri.csv\"  # Replace with the data source \n",
    "ontology_file = \"foo.ttl\" # Replace with the ontology\n",
    "\n",
    "# Create an RDF graph\n",
    "graph = Graph()\n",
    "\n",
    "# Load the ontology into the graph\n",
    "graph.parse(source=ontology_file, format=\"ttl\")\n",
    "\n",
    "# Set the namespace for your ontology\n",
    "foo = Namespace(\"https://w3id.org/def/foo#\")\n",
    "sosa = Namespace(\"http://w3.org/ns/sosa/\")\n",
    "pos = Namespace(\"https://w3.org/2003/01/geo/wgs84_pos#\")\n",
    "xsd= Namespace('http://www.w3.org/2001/XMLSchema#')\n",
    "# RML mapping code\n",
    "# Iterate over the CSV file and map the data to RDF triples\n",
    "with open(csv_file, 'r') as file:\n",
    "    # Skip the header row if present\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        # Split the CSV line into columns\n",
    "        columns = line.strip().split(',')\n",
    "        \n",
    "\n",
    "        # Create subject URI\n",
    "        subject_uri = URIRef(foo+columns[0])\n",
    "\n",
    "        # Add triples to the graph\n",
    "        graph.add((subject_uri, RDF.type, sosa.Observation)) # Replace with the appropriate class from your ontology\n",
    "        graph.add((subject_uri, foo.LocalDate, Literal(columns[1], datatype=xsd.date))) # Replace with the appropriate predicate from your ontology\n",
    "        graph.add((subject_uri, foo.LocalTime, Literal(columns[2], datatype=xsd.time)))\n",
    "        graph.add((subject_uri, foo.GMTDate, Literal(columns[3], datatype=xsd.date)))\n",
    "        graph.add((subject_uri, foo.GMTTime, Literal(columns[4], datatype=xsd.time)))\n",
    "        graph.add((subject_uri, pos.lat, Literal(columns[5], datatype=xsd.float)))\n",
    "        graph.add((subject_uri, pos.long, Literal(columns[6], datatype=xsd.float)))\n",
    "        graph.add((subject_uri, foo.Temperature, Literal(columns[8], datatype=xsd.double)))\n",
    "        graph.add((subject_uri, foo.Speed, Literal(columns[9], datatype=xsd.integer)))\n",
    "        graph.add((subject_uri, foo.Direction, Literal(columns[10], datatype=xsd.integer)))\n",
    "        graph.add((subject_uri, foo.Cov, Literal(columns[11], datatype=xsd.integer)))\n",
    "        graph.add((subject_uri, foo.HDOP, Literal(columns[12], datatype=xsd.integer)))\n",
    "        graph.add((subject_uri, foo.Distance, Literal(columns[13], datatype=xsd.float)))\n",
    "      \n",
    "\n",
    "# Save the resulting knowledge graph to a file\n",
    "output_file = \"SeriKG.rdf\"\n",
    "graph.serialize(destination=output_file, format=\"ttl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
