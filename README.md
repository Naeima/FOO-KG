## Forest Observatory Ontology Knowledge Graph (FOO-KG)

[FOO-KG](https://naeima.github.io/fooKG/) is a wildlife knowledge graph dataset used for poaching prediction. As shown in the figure below, it was constructed by populating the Forest Observatory Ontology [FOO](https://w3id.org/def/foo#) with three structured wildlife datasets generated by sensors. This was achieved by mapping each data table to the sensor class, each column to a property or predicate, each row to an instance or individual of the class sensor, and each cell to a literal using [YARRML](https://rml.io/yarrrml/), a human-readable text-based representation for declarative Linked Data generation rules. The output RDF graphs are then fed into a graph database and merged with FOO for integration and reasoning.

![image](https://lucid.app/publicSegments/view/5eec075d-5ace-4ec0-8165-85ea232301a2/image.png)


## FOO-KG (a) Jupyter Notebook

This Jupyter notebook, titled "FOO-KG (a)", is designed for processing and analysing RDF (Resource Description Framework) data. The notebook focuses on loading RDF data, querying it using SPARQL, extracting features and labels, and preparing the data for further analysis or predictive modelling. This notebook is particularly useful for those working with geospatial RDF data and looking to predict elephants' geolocation.

## Requirements

To run this notebook, you will need the following:

- Python 3.x
- Jupyter Notebook or Jupyter Lab environment
- The following Python libraries:
  - `rdflib` for handling RDF data
  - `numpy` and `pandas` for data manipulation
  - `tensorflow` and `sklearn` for machine learning tasks
  - `networkx` for network analysis (if applicable)
  - Other dependencies as required by the code (refer to the first cell of the notebook for all imports)

## Setup

1. **Install Python and Jupyter**: Ensure you have Python 3.x installed along with Jupyter Notebook or Jupyter Lab.

2. **Clone/Download the Notebook**: Obtain a copy of the "FOO-KG (a)" notebook onto your local machine.

3. **Install Required Libraries**: You can install the required libraries using pip. For example:

   ```
   pip install rdflib numpy pandas tensorflow sklearn networkx
   ```

4. **Download RDF Data**: Ensure you have the RDF data file (e.g., "SeriKG.rdf") available and in the correct format as expected by the notebook.

## Usage

1. **Open the Notebook**: Open the "FOO-KG (a)" notebook in your Jupyter environment.

2. **Run Each Cell**: Execute each cell sequentially. The initial cells import necessary libraries and define functions.

3. **Load and Query RDF Data**: The notebook will load RDF data from a file and perform a SPARQL query to extract relevant features and labels.

4. **Data Analysis and Modeling**: Follow the subsequent cells for data preprocessing, analysis, and potentially building machine learning models.

## Additional Notes

- The notebook assumes some familiarity with RDF data, SPARQL queries, and Python programming.
- You may need to modify the SPARQL query and data paths according to your specific RDF dataset.
- Ensure that all dependencies are correctly installed and imported in the notebook.


## FOO-KG for Poaching Prediction 

The figure below illustrates a framework that is a complete predictive system that combines RDF data extraction and deep learning to enable precise prediction. It includes a Keras sequential neural network for predicting the geo-location of elephants and a GNN model for predicting poaching activities using a subset of the knowledge graph. The performance of the models was evaluated using the Root Mean Square Error (RMSE) metric, and accurate predictions were transformed back to their original RDF format. This integration of semantic web data and machine learning allows for more accurate and informed predictions.

![image](https://lucid.app/publicSegments/view/52ed0585-a337-482a-8e30-12473953eb82/image.png)

The Jupyter notebook titled "FOO-KG (b)" seems to be focused on processing RDF data and possibly building a machine learning model using that data. Here's a brief overview of the first few cells:

1. **Importing Libraries and Dependencies**: The first cell imports several libraries such as `numpy`, `pandas`, `rdflib` for RDF data handling, `networkx`, and `tensorflow` for machine learning, as well as other dependencies related to data processing and model building.

2. **Loading RDF Data**: The second cell loads RDF data from a file named "Graph.ttl" using `rdflib`.

3. **Data Preparation**: The third cell iterates over triples in the RDF graph, extracting subjects, predicates, and objects, and stores them in a pandas DataFrame. It also saves this DataFrame to a CSV file named 'rdf_data.csv'.

Based on this information, I'll draft a README file explaining how to use this notebook. The README will include sections like Introduction, Requirements, Setup, Usage, and Additional Notes. Let's proceed with creating the README for "FOO-KG (b)".

## FOO-KG (b) Jupyter Notebook

The "FOO-KG (b)" Jupyter notebook is designed for advanced data processing and analysis of RDF (Resource Description Framework) data, potentially leading to the development of machine learning models. The primary focus of this notebook is on loading RDF data, transforming it into a structured format, and preparing it for analysis or machine learning tasks. This tool is particularly useful for those working with complex RDF datasets and interested in applying data science and machine learning techniques.

## Requirements

To effectively use this notebook, you will need:

- Python 3.x
- Jupyter Notebook or Jupyter Lab environment
- Required Python libraries:
  - `rdflib` for RDF data manipulation
  - `numpy` and `pandas` for data handling
  - `tensorflow` for building and training machine learning models
  - `networkx` for network analysis (if applicable)
  - `sklearn` for model evaluation and preprocessing
  - Additional dependencies as specified in the notebook

## Setup

1. Python and Jupyter Installation: Ensure Python 3.x is installed along with Jupyter Notebook or Jupyter Lab.

2. Acquire the Notebook: Download or clone the "FOO-KG (b)" notebook to your local environment.

3. Library Installation: Use pip to install the required libraries. Example command:

   ```
   pip install rdflib numpy pandas tensorflow networkx sklearn
   ```

4. RDF Data File: Make sure you have the RDF data file (like "Graph.ttl") in the correct format as expected by the notebook.

## Usage

1. Open the Notebook: Launch the "FOO-KG (b)" notebook in your Jupyter environment.

2. Sequential Execution: Run each cell in order, starting from the top. The first cells deal with importing necessary libraries and setting up the environment.

3. Load and Process RDF Data: The notebook includes cells to load RDF data and transform it into a structured DataFrame.

4. Data Analysis and Model Development: Follow the instructions in the notebook to preprocess the data and develop and train machine learning models.

## Additional Notes

- Familiarity with RDF data structures, Python programming, and basic machine learning concepts is beneficial for using this notebook.
- You might need to adjust the RDF data loading path and the structure of SPARQL queries based on your specific dataset.
- Ensure all dependencies are correctly installed and that the notebook is compatible with your system configuration.

---
Based on a deeper analysis of the "FOO-KG (b)" notebook, here is a more detailed description of its contents and functionalities:

1. Data Encoding and Preparation: After loading the RDF data, the notebook encodes subjects, predicates, and objects from the RDF triples into numerical formats. This is achieved by creating dictionaries for encoding each element and then preparing input tensors for each of the encoded elements (subjects, predicates, objects).

2. Graph Neural Network Model Definition: The notebook defines a custom Graph Neural Network (GNN) class named `GraphNetwork`, which is a TensorFlow model. This model includes embedding layers for nodes and relations, a graph convolution layer, and a dense output layer. The model is intended for tasks like node classification or link prediction in the RDF graph.

3. Model Instantiation and Compilation: The notebook then instantiates and compiles the `GraphNetwork` model, specifying parameters such as the number of nodes, number of relations, and embedding dimensions. The model is compiled with the Adam optimizer and binary cross-entropy loss function, indicating a potential binary classification task.

4. Model Training: The model is trained on the prepared dataset, with input tensors corresponding to subjects, predicates, and objects, and a binary label. The training process involves several epochs, and the performance is evaluated on a validation set.

5. Additional Processing and Analysis: The notebook includes further cells for analyzing the model's performance, such as plotting training and validation metrics, evaluating test set performance, or performing additional data analyses.

